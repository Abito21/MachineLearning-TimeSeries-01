# -*- coding: utf-8 -*-
"""Sub2_BelajarPengembanganML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11MHIu11XeEgcS8xNyjU4WOx2BEOpzTot

Submission 2 - Belajar Pengembangan Machine Learning Dicoding

Menggunakan dataset delhi weather data yang diunduh melalui Kaggle pada melalui link https://www.kaggle.com/mahirkukreja/delhi-weather-data
"""

# Mengambil data melalui google drive
# Dengan melakukan autentikasi akun

from google.colab import drive
drive.mount('/content/drive')

!ls '/content/drive/My Drive/Dataset Latihan Pengembang ML'

# Import library pandas
# df singkatan dari dataframe

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Dataset Latihan Pengembang ML/testset.csv')
df.info()

# Mengetahui ukuran data dengan shape
# Terdiri dari (baris, kolom)

df.shape # Dataset terdiri dari 100990 baris data dan 20 kolom atribut

df.head()

# Mengecek nilai kosong / nol pada dataset

df.isnull().sum()

# Melakukab preprocessing data
# Mengubah data tanggal dari string menjadi data datetime

df['datetime_utc']=pd.to_datetime(df['datetime_utc'])
df['datetime_utc'].head()

# Normalisasi data yang melebihi 50C karena tidak relevan
import numpy as np

median = df.loc[df[' _tempm'] < 55, ' _tempm'].median()
df.loc[df[' _tempm'] > 55, ' _tempm'] = np.nan
df.fillna(median,inplace=True)

# Mengisi nilai no dengan nilai rata-rata dari data
df[' _tempm'].fillna(df[' _tempm'].mean(), inplace=True)
df = df[['datetime_utc',' _tempm' ]]

df.head()

# Melakukan resample untuk kolom datetime

df['datetime'] = df['datetime_utc']
df_datetime = df
df_datetime = df[['datetime_utc', 'datetime']]
df_datetime = df_datetime.set_index('datetime_utc')
df_datetime.head(10)
df_datetime = df_datetime['datetime'].resample('H').ffill()
df_datetime.head(10)

# Mengubah kolom datetime menjadi index pada data

df_tempm = df.set_index('datetime_utc')
df_tempm.head()

# Data temperature setelah dilakukan resample() untuk per jam

df_tempm = df_tempm[' _tempm'].resample('H').ffill()
df_tempm.head(10)

# Dataset Temperature di kota Delhi setelah dilakukan preprocessing

df = pd.concat([df_datetime, df_tempm], axis = 1)
df.head(10)

# Ukuran dataset temperatur kota Delhi yang sudah dilakukan preprocessing

df.shape

# df = df.set_index([pd.Index([1..179504])])
# df.head(10)

# Membuat nilai untuk memisahkan antara data tanggal dan data temperatur

tanggal = df['datetime'].values
tempt  = df[' _tempm'].values

# Import library untuk mengelola model dan visualisasi

from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.python.ops.gen_dataset_ops import dataset_to_graph
from sklearn.model_selection import train_test_split

# Visualisasi Nilai rata - rata temperatur

plt.figure(figsize=(15,5))
plt.plot(tanggal, tempt)
plt.title('Rata - rata Temperatur',
          fontsize=20);

# Fungsi untuk mengatur nilai batch size

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

# Memisahkan data untuk latih dan uji menggunakan train_test_split

x_latih, x_uji, y_latih, y_uji = train_test_split(tempt, tanggal, test_size = 0.2, random_state = 0 , shuffle=False)
print(len(x_latih), len(x_uji))

train_set = windowed_dataset(x_latih, window_size=60, batch_size=100, shuffle_buffer=4000)
test_set = windowed_dataset(x_uji, window_size=60, batch_size=100, shuffle_buffer=4000)

# Build Model Machine Learning untuk Time Series

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 400),
])

# Untuk mengetahui ketentuan MAE maka didasarkan pada rentang data, seperti berikut
# z = (data_max - data_min)*0.1, variabel yang digunakan untuk membandingkan diumpamakan 'z'

data_max = df[' _tempm'].max()
print('Nilai maksimal : ' )
print(data_max)

data_min = df[' _tempm'].min()
print('\nNilai minimal : ' )
print(data_min)

# Data nilai perbandingan untuk mengetahui nilai MAE < 10%

z = ((data_max - data_min) * 0.1)
print('Nilai perbandingan MAE : ')
print(z)

"""Dengan kata lain nilai MAE yang dimaksud harus dibawah 8.9 tersebut."""

# Fungsi Callback untuk menghentikan training model pada 85%

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('mae')< 10.0):
      self.model.stop_training = True
      print("\nMAE daripada model < 10% dari skala data")
callbacks = myCallback()

# Atur learning rate
lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch / 20))

# Atur parameter optimizer
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)

# Model Compile
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

# Membuar variabel history untuk dapat divisualisasikan

history = model.fit(train_set, epochs=100, validation_data= test_set, callbacks=[callbacks])

# Visualisasi Plot Loss Model

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

# Visualisasi Plot MAE Model

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model MAE')
plt.ylabel('Mae')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()